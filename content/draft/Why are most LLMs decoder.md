---
banner: 
method: 
title: Why are most LLMs decoder
description: Dive into the rabbit hole of recent advancement in Large Language Models
audience: 
latar belakang: 
tujuan: 
platfom: Medium Himit Pens, Personal Blog
tone: Persuasive, Information
language: English
date: 
tags: ["data science", "machine learning", "ai", "llm"]
published: false
---
I came across this question during mentoring for DeltaHacks and could not come up with an answer that was persuasive enough for myself. So I did some digging and it turned out to be a fascinating rabbit hole to dive down into. It involves a blend of understanding transformers, architectures, mathematics, and engineering optimizations. Here I will share the two cents that I picked up along the way.

Reference:
https://medium.com/@yumo-bai/why-are-most-llms-decoder-only-590c903e4789